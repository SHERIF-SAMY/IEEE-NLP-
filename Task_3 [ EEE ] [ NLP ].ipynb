{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6a2c0a",
   "metadata": {},
   "source": [
    "# Text Preprocessing and Word2Vec Representation\n",
    "## Agenda:\n",
    "* 1- **Introduction** - Overview of text preprocessing and Word2Vec.\n",
    "* 2- **Arabic Text Preprocessing** - Explanation and implementation.\n",
    "* 3- **English Text Preprocessing** - Explanation and implementation.\n",
    "* 4- **Word2Vec Representation** - Explanation and implementation.\n",
    "* 5- **Embedding Visualization** - Applying t-SNE for 2D scatter plot.\n",
    "* 6- **Summary** - Reflection and key takeaways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e78cf",
   "metadata": {},
   "source": [
    "# 1- **Introduction**\n",
    "In this notebook,\n",
    "> 1- **we will preprocess two texts**,\n",
    " \n",
    " >> 1- arabic_text = \"Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… ÙÙŠ AIØŒ Ø§Ù†Ù‡ Ù…Ù…ØªØ¹ Ø¬Ø¯Ø§ ÙˆØ±Ø§Ø¦Ø¹! ðŸ˜‰ Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.\"\n",
    " \n",
    " >> 2- english_text = \"Hello, welcome to AI! It's super fun & amaizing :) This is a test text.\"\n",
    "\n",
    " > 2- **generate Word2Vec embeddings**\n",
    " \n",
    " > 3- **visualize them using t-SNE**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c82c7f",
   "metadata": {},
   "source": [
    "# **2- Arabic Text Preprocessing**\n",
    "> Arabic text often contains diacritics, punctuation, and a mix of English and Arabic words. We'll normalize, tokenize, remove stopwords, and correct spelling issues to prepare the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb8ab4",
   "metadata": {},
   "source": [
    "## 2.1- Arabic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f3302cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farasapy in d:\\anaconda\\lib\\site-packages (0.0.14)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from farasapy) (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from farasapy) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->farasapy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->farasapy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->farasapy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->farasapy) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->farasapy) (0.4.6)\n",
      "Requirement already satisfied: pyspellchecker in d:\\anaconda\\lib\\site-packages (0.8.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "! pip install farasapy\n",
    "!pip install pyspellchecker\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Sample Arabic text\n",
    "arabic_text = \"Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… ÙÙŠ AIØŒ Ø§Ù†Ù‡ Ù…Ù…ØªØ¹ Ø¬Ø¯Ø§ ÙˆØ±Ø§Ø¦Ø¹! ðŸ˜‰ Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b16ae84",
   "metadata": {},
   "source": [
    "## 2.2- Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7697038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… ÙÙŠ AIØŒ Ø§Ù†Ù‡ Ù…Ù…ØªØ¹ Ø¬Ø¯Ø§ ÙˆØ±Ø§Ø¦Ø¹! ðŸ˜‰ Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.\n",
      "Ù…Ø±Ø­Ø¨Ø§ Ø¨ÙƒÙ… ÙÙŠ ai Ø§Ù†Ù‡ Ù…Ù…ØªØ¹ Ø¬Ø¯Ø§ ÙˆØ±Ø§Ø¡Ø¹  Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n"
     ]
    }
   ],
   "source": [
    "def normalize_arabic(text):\n",
    "    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n",
    "    text = re.sub(r'Ø¡|Ø¦|Ø¤', 'Ø¡', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "normalized_arabic_text = normalize_arabic(arabic_text)\n",
    "print(arabic_text)\n",
    "print(normalized_arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e94c8",
   "metadata": {},
   "source": [
    "## 2.3- Tokenize and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72b70318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… ÙÙŠ AIØŒ Ø§Ù†Ù‡ Ù…Ù…ØªØ¹ Ø¬Ø¯Ø§ ÙˆØ±Ø§Ø¦Ø¹! ðŸ˜‰ Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.\n",
      "['Ù…Ø±Ø­Ø¨Ø§', 'ai', 'Ø§Ù†Ù‡', 'Ù…Ù…ØªØ¹', 'Ø¬Ø¯Ø§', 'ÙˆØ±Ø§Ø¡Ø¹', 'Ù†Øµ', 'ØªØ¬Ø±ÙŠØ¨ÙŠ', 'Ø¨Ø§Ù„Ù„ØºØ©', 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "arabic_tokens = word_tokenize(normalized_arabic_text)\n",
    "\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "cleaned_arabic_tokens = [word for word in arabic_tokens if word not in arabic_stopwords]\n",
    "\n",
    "print(arabic_text)\n",
    "print(cleaned_arabic_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0daa91",
   "metadata": {},
   "source": [
    "## 2.4- Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd81373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Arabic Tokens: ['Ù…Ø±Ø­Ø¨Ø§', 'Ù…Ù†', 'Ø§Ù†Ù‡', 'Ù…Ù…ØªØ¹', 'Ø¬Ø¯Ø§', 'ÙˆØ±Ø§Ø¡', 'Ù†Øµ', 'ØªØ¬Ø±ÙŠØ¨ÙŠ', 'Ø¨Ø§Ù„Ù„ØºØ©', 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©']\n"
     ]
    }
   ],
   "source": [
    "spell = SpellChecker(language='ar')\n",
    "corrected_arabic_tokens = [spell.correction(word) for word in cleaned_arabic_tokens]\n",
    "\n",
    "print(\"Cleaned Arabic Tokens:\", corrected_arabic_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc48484",
   "metadata": {},
   "source": [
    "# 3- **English Text Preprocessing**\n",
    "> English text issues include contractions, slang, and punctuation problems. We'll normalize, tokenize, remove stopwords, and correct spelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6eb4e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample English text\n",
    "english_text = \"Hello, welcome to AI! It's super fun & amaizing :) This is a test text.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718063f8",
   "metadata": {},
   "source": [
    "## 3.2- Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e62fc8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, welcome to AI! It's super fun & amaizing :) This is a test text.\n",
      "hello welcome to ai its super fun  amaizing  this is a test text\n"
     ]
    }
   ],
   "source": [
    "def normalize_english(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "normalized_english_text = normalize_english(english_text)\n",
    "print(english_text)\n",
    "print(normalized_english_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64762aec",
   "metadata": {},
   "source": [
    "## 3.3- Tokenize and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5eddd475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello welcome to ai its super fun  amaizing  this is a test text\n",
      "['hello', 'welcome', 'ai', 'super', 'fun', 'amaizing', 'test', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "english_tokens = word_tokenize(normalized_english_text)\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "cleaned_english_tokens = [word for word in english_tokens if word not in english_stopwords]\n",
    "\n",
    "print(normalized_english_text)\n",
    "print(cleaned_english_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625fea5",
   "metadata": {},
   "source": [
    "## 3.4- Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "073f4553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned English Tokens: ['hello', 'welcome', 'ai', 'super', 'fun', 'amazing', 'test', 'text']\n"
     ]
    }
   ],
   "source": [
    "spell = SpellChecker()\n",
    "corrected_english_tokens = [spell.correction(word) for word in cleaned_english_tokens]\n",
    "\n",
    "print(\"Cleaned English Tokens:\", corrected_english_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7aaee1",
   "metadata": {},
   "source": [
    "# **4- Word2Vec Representation\n",
    "> Word2Vec helps capture word semantics by mapping words into a vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f03d2-5666-4d69-ba39-bc14bfd0ce32",
   "metadata": {},
   "source": [
    "<img src=\"C:\\Users\\Asus\\Desktop\\tf idf .png\" alt=\"Image Alt Text\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15242552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in d:\\anaconda\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: gensim in d:\\anaconda\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in d:\\anaconda\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in d:\\anaconda\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\anaconda\\lib\\site-packages (from gensim) (5.2.1)\n",
      "\n",
      "Arabic Words and Embeddings:\n",
      " ['Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©', 'Ø¨Ø§Ù„Ù„ØºØ©', 'ØªØ¬Ø±ÙŠØ¨ÙŠ', 'Ù†Øµ', 'ÙˆØ±Ø§Ø¡']\n",
      "\n",
      "English Words and Embeddings:\n",
      " ['text', 'test', 'amazing', 'fun', 'super']\n"
     ]
    }
   ],
   "source": [
    "! pip install scipy\n",
    "! pip install gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec models\n",
    "arabic_model = Word2Vec([corrected_arabic_tokens], vector_size=100, window=5, min_count=1, workers=4)\n",
    "english_model = Word2Vec([corrected_english_tokens], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Extract embeddings\n",
    "arabic_words = list(arabic_model.wv.index_to_key)[:5]\n",
    "english_words = list(english_model.wv.index_to_key)[:5]\n",
    "\n",
    "print(\"\\nArabic Words and Embeddings:\\n\", arabic_words)\n",
    "# print([arabic_model.wv[word] for word in arabic_words])  # ---> vector\n",
    "\n",
    "print(\"\\nEnglish Words and Embeddings:\\n\", english_words)\n",
    "# print([english_model.wv[word] for word in english_words])  # ---> vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2a28f-4665-4253-a068-fc20b055545e",
   "metadata": {},
   "source": [
    "# **5- TF IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c227b9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English TF-IDF Feature Names:\n",
      " ['ai' 'amaizing' 'fun' 'hello' 'super' 'test' 'text' 'welcome']\n",
      "English TF-IDF Values:\n",
      " [[0.35355339 0.35355339 0.35355339 0.35355339 0.35355339 0.35355339\n",
      "  0.35355339 0.35355339]]\n",
      "\n",
      "Arabic TF-IDF Feature Names:\n",
      " ['ai' 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' 'Ø§Ù†Ù‡' 'Ø¨Ø§Ù„Ù„ØºØ©' 'ØªØ¬Ø±ÙŠØ¨ÙŠ' 'Ø¬Ø¯Ø§' 'Ù…Ø±Ø­Ø¨Ø§' 'Ù…Ù…ØªØ¹' 'Ù†Øµ' 'ÙˆØ±Ø§Ø¡Ø¹']\n",
      "Arabic TF-IDF Values:\n",
      " [[0.31622777 0.31622777 0.31622777 0.31622777 0.31622777 0.31622777\n",
      "  0.31622777 0.31622777 0.31622777 0.31622777]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Cleaned tokens for English and Arabic\n",
    "# cleaned_english_tokens = ['hello', 'welcome', 'ai', 'super', 'fun', 'amazing', 'test', 'text']\n",
    "# cleaned_arabic_tokens = ['Ù…Ø±Ø­Ø¨Ø§', 'Ù…Ù†', 'Ø§Ù†Ù‡', 'Ù…Ù…ØªØ¹', 'Ø¬Ø¯Ø§', 'ÙˆØ±Ø§Ø¡', 'Ù†Øµ', 'ØªØ¬Ø±ÙŠØ¨ÙŠ', 'Ø¨Ø§Ù„Ù„ØºØ©', 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©']\n",
    "\n",
    "english_doc = ' '.join(cleaned_english_tokens)  \n",
    "arabic_doc = ' '.join(cleaned_arabic_tokens)\n",
    "\n",
    "# Create TfidfVectorizer instances\n",
    "tfidf_vectorizer_english = TfidfVectorizer()\n",
    "tfidf_vectorizer_arabic = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the \"documents\"\n",
    "english_tfidf = tfidf_vectorizer_english.fit_transform([english_doc])\n",
    "arabic_tfidf = tfidf_vectorizer_arabic.fit_transform([arabic_doc])\n",
    "\n",
    "# Output results\n",
    "print(\"English TF-IDF Feature Names:\\n\", tfidf_vectorizer_english.get_feature_names_out())\n",
    "print(\"English TF-IDF Values:\\n\", english_tfidf.toarray())\n",
    "\n",
    "print(\"\\nArabic TF-IDF Feature Names:\\n\", tfidf_vectorizer_arabic.get_feature_names_out())\n",
    "print(\"Arabic TF-IDF Values:\\n\", arabic_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f3ea3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "english_df = pd.DataFrame(english_tfidf.toarray(), columns=tfidf_vectorizer_english.get_feature_names_out())\n",
    "arabic_df = pd.DataFrame(arabic_tfidf.toarray(), columns=tfidf_vectorizer_arabic.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37e03fc5-f0e5-4ef9-9652-e3e5d72a237f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>amaizing</th>\n",
       "      <th>fun</th>\n",
       "      <th>hello</th>\n",
       "      <th>super</th>\n",
       "      <th>test</th>\n",
       "      <th>text</th>\n",
       "      <th>welcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ai  amaizing       fun     hello     super      test      text  \\\n",
       "0  0.353553  0.353553  0.353553  0.353553  0.353553  0.353553  0.353553   \n",
       "\n",
       "    welcome  \n",
       "0  0.353553  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc5a0d76-e1ed-4e94-b378-683ebb037632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</th>\n",
       "      <th>Ø§Ù†Ù‡</th>\n",
       "      <th>Ø¨Ø§Ù„Ù„ØºØ©</th>\n",
       "      <th>ØªØ¬Ø±ÙŠØ¨ÙŠ</th>\n",
       "      <th>Ø¬Ø¯Ø§</th>\n",
       "      <th>Ù…Ø±Ø­Ø¨Ø§</th>\n",
       "      <th>Ù…Ù…ØªØ¹</th>\n",
       "      <th>Ù†Øµ</th>\n",
       "      <th>ÙˆØ±Ø§Ø¡Ø¹</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ai   Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©       Ø§Ù†Ù‡    Ø¨Ø§Ù„Ù„ØºØ©    ØªØ¬Ø±ÙŠØ¨ÙŠ       Ø¬Ø¯Ø§     Ù…Ø±Ø­Ø¨Ø§  \\\n",
       "0  0.316228  0.316228  0.316228  0.316228  0.316228  0.316228  0.316228   \n",
       "\n",
       "       Ù…Ù…ØªØ¹        Ù†Øµ     ÙˆØ±Ø§Ø¡Ø¹  \n",
       "0  0.316228  0.316228  0.316228  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7bdcc-44f9-453b-b39e-c1940cff191a",
   "metadata": {},
   "source": [
    "# **Summary**\n",
    "> In this notebook, we:\n",
    "\n",
    ">>Preprocessed Arabic and English texts by removing noise and normalizing the content.\n",
    "Trained Word2Vec models to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a89947-b74b-456f-9cbc-89fd2ae17f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
